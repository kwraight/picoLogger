{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import influxdb_client\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from influxdb_client.client.exceptions import InfluxDBError\n",
    "from getpass import getpass\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set influx connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get influx credentials from file (not in repo., see template)\n",
    "thisDir=os.getcwd()\n",
    "if os.path.isdir(thisDir):\n",
    "    print(\"directory found:\",thisDir)\n",
    "    sys.path.insert(1, thisDir)\n",
    "    import connectionDetails\n",
    "    connectionDict=connectionDetails.GetInfluxCredentials()\n",
    "    #print(credDict)\n",
    "    print(\"done.\")\n",
    "else:\n",
    "    print(\"no directory found:\",thisDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set credentials\n",
    "bucket_remote = connectionDict['bucket']\n",
    "org_remote = connectionDict['org']\n",
    "token_remote = connectionDict['token']\n",
    "url_remote = connectionDict['url']\n",
    "\n",
    "print(\"Connection details:\")\n",
    "print(f\"bucket_remote: {bucket_remote}\")\n",
    "print(f\"org_remote: {org_remote}\")\n",
    "print(f\"token_remote: {token_remote}\")\n",
    "print(f\"url_remote: {url_remote}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data\n",
    "\n",
    "The code below takes in the flux data as well as the data from the picolog folder to perform calculations on them and produce an output in the csv file 'output_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### geting temp., humid. data from influx\n",
    "def fetch_temperature_and_humidity(start_time_str, bucket_remote, url_remote, token_remote, org_remote):\n",
    "    clientV2_remote = influxdb_client.InfluxDBClient(url=url_remote, token=token_remote, org=org_remote)\n",
    "    query_api_remote = clientV2_remote.query_api()\n",
    "\n",
    "    start_time = datetime.strptime(str(start_time_str), '%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "    filters = {'_measurement': \"data\"}\n",
    "    query_str = f'from(bucket: \\\"{bucket_remote}\\\") |> range(start: {start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")})'\n",
    "    for k, v in filters.items():\n",
    "        query_str += f' |> filter(fn: (r) => r[\"{k}\"] == \"{v}\")'\n",
    "    query_str += ' |> yield(name: \"mean\")'\n",
    "\n",
    "    query_result_remote = query_api_remote.query_data_frame(org=org_remote, query=query_str)\n",
    "\n",
    "    temperature_results = []\n",
    "    humidity_results = []\n",
    "    for index, row in query_result_remote.iterrows():\n",
    "        if row['_field'] == 'temperature':\n",
    "            temperature_results.append(row.to_dict())\n",
    "        elif row['_field'] == 'humidity':\n",
    "            humidity_results.append(row.to_dict())\n",
    "\n",
    "    if len(temperature_results) > 0:\n",
    "        temperature = temperature_results[0]['_value']\n",
    "    else:\n",
    "        temperature = None\n",
    "\n",
    "    if len(humidity_results) > 0:\n",
    "        humidity = humidity_results[0]['_value']\n",
    "    else:\n",
    "        humidity = None\n",
    "\n",
    "    return temperature, humidity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### derive values from picoLogger raw data\n",
    "def perform_calculations(input_folder, output_file, bucket_remote, url_remote, token_remote, org_remote, timestamp_str):\n",
    "    all_data = []\n",
    "    \n",
    "    # Convert the timestamp to the desired format\n",
    "    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d_%H:%M:%S\").strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Calculate Vin Drop and GND Drop\n",
    "            data['Vin Drop (V)'] = data['Vin+ Last (V)'] - data['Vin- Last (V)']\n",
    "            data['GND Drop (V)'] = data['GND+ Last (V)'] - data['GND- Last (V)']\n",
    "            \n",
    "            # Calculate Resistance Vin(Ohms) and Total Resistance(mOhms)\n",
    "            data['Resistance Vin(Ohms)'] = data['Vin Drop (V)'] / 10 / 5\n",
    "            data['Resistance GND(Ohms)'] = data['GND Drop (V)'] / 10 / 5\n",
    "            data['Total Resistance(mOhms)'] = (data['Resistance Vin(Ohms)'] + data['Resistance GND(Ohms)']) * 1000\n",
    "            \n",
    "            # Calculate Capacitor Equivalent leakage current(nA)\n",
    "            data['Capacitor Equivalent leakage current(nA)'] = ((data['Capacitor leakage test Last (V)'] / 10) / (1 * 10 ** 6)) / 1000000\n",
    "            \n",
    "            # Calculate NTC value(Kohms)\n",
    "            data['NTC value (Kohms)'] = 0.2 * 51 / data['NTC Last (V)']\n",
    "            \n",
    "            # Fetch temperature and humidity\n",
    "            temperature, humidity = fetch_temperature_and_humidity(timestamp_str, bucket_remote, url_remote, token_remote, org_remote)\n",
    "            \n",
    "            # Prepare the new DataFrame with desired columns and values\n",
    "            result_data = pd.DataFrame({\n",
    "                'component': filename,\n",
    "                'componentType': 'PCB',\n",
    "                'stage': 'PCB_QC',\n",
    "                'testType': 'HV_LV_TEST',\n",
    "                'institution': 'Glasgow',\n",
    "                'runNumber': data.shape[0],\n",
    "                'date': date.today().strftime('%Y-%m-%d'),\n",
    "                'passed': 'YES',\n",
    "                'problems': 'False',\n",
    "                'property1_value': 'B.masic',\n",
    "                'property1_key': 'OPERATOR',\n",
    "                'property2_value': 'INSTRUMENT',\n",
    "                'property2_key': '',\n",
    "                'property3_value': 'ANALYSIS_VERSION',\n",
    "                'property3_key': '',\n",
    "                'result1_key': 'VIN_DROP',\n",
    "                'result1_value': data['Vin Drop (V)'],\n",
    "                'result2_key': 'GND_DROP',\n",
    "                'result2_value': data['GND Drop (V)'],\n",
    "                'result3_key': 'EFFECTIVE RESISTANCE',\n",
    "                'result3_value': data['Total Resistance(mOhms)'],\n",
    "                'result4_key': 'HV_LEAKAGE',\n",
    "                'result4_value': data['Capacitor leakage test Last (V)'],\n",
    "                'result5_key': 'LEAKAGE_CURRENT (nA)',\n",
    "                'result5_value': data['Capacitor Equivalent leakage current(nA)'],\n",
    "                'result6_key': 'NTC_VOLTAGE',\n",
    "                'result6_value': data['NTC Last (V)'],\n",
    "                'result7_key': 'NTC_VALUE',\n",
    "                'result7_value': data['NTC value (Kohms)'],\n",
    "                'result8_key': 'TERMPERATURE',\n",
    "                'result8_value': temperature,\n",
    "                'result9_key': 'HUMIDITY',\n",
    "                'result9_value': humidity,\n",
    "                'timestamp': [timestamp]  # Add the timestamp column to the DataFrame\n",
    "            })\n",
    "            \n",
    "            all_data.append(result_data)\n",
    "    \n",
    "    if all_data:\n",
    "        final_result = pd.concat(all_data, ignore_index=True)\n",
    "        final_result.to_csv(output_file, index=False)\n",
    "        print(f\"Calculation completed. Output saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run conversion\n",
    "input_folder = thisDir+\"/../picolog_folder\"\n",
    "output_file = thisDir+\"/../output_file.csv\"\n",
    "timestamp_str = \"2023-08-01_23:00:00\"  # Replace this with your desired timestamp\n",
    "\n",
    "perform_calculations(input_folder, output_file, bucket_remote, url_remote, token_remote, org_remote, timestamp_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
